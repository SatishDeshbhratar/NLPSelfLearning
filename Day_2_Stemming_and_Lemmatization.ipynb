{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatishDeshbhratar/NLPSelfLearning/blob/main/Day_2_Stemming_and_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "89515d5f9131c2e4b35d969ab43d372c19763db7",
        "id": "e8Fma8SpDW2c"
      },
      "source": [
        "# Day 2 - Stemming and Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming Words Code Examples\n",
        "\n",
        "StemmingÂ usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes."
      ],
      "metadata": {
        "id": "R_lcw1jxFvkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPXc876DW2e",
        "outputId": "8f52ee4b-18c4-4c56-85e9-6baf3d46d42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer\n",
            "cats =>  cat\n",
            "trouble =>  troubl\n",
            "troubling => troubl\n",
            "troubled =>  troubl\n",
            "Lancaster Stemmer\n",
            "cats =>  cat\n",
            "trouble =>  troubl\n",
            "troubling => troubl\n",
            "troubled =>  troubl\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "#create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "#provide a word to be stemmed\n",
        "print(\"Porter Stemmer\")\n",
        "print(\"cats => \",porter.stem(\"cats\"))\n",
        "print(\"trouble => \",porter.stem(\"trouble\"))\n",
        "print(\"troubling =>\", porter.stem(\"troubling\"))\n",
        "print(\"troubled => \",porter.stem(\"troubled\"))\n",
        "print(\"Lancaster Stemmer\")\n",
        "print(\"cats => \",lancaster.stem(\"cats\"))\n",
        "print(\"trouble => \",lancaster.stem(\"trouble\"))\n",
        "print(\"troubling =>\",lancaster.stem(\"troubling\"))\n",
        "print(\"troubled => \",lancaster.stem(\"troubled\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "84b05c4bfe6394ee3ef2c319f3e125e1faf85536",
        "id": "sHRMJkLkDW2e"
      },
      "source": [
        "### Stemming a  Complete Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_uuid": "a909dc83f715a806ad551b0e7acea95097d92147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2qYB58ODW2f",
        "outputId": "691b2978-3e1a-4b24-f8f6-df965563418a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "x=stemSentence(sentence)\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "aa6da7abea37807bc9a00a1bc8ae5034fe73b07b",
        "id": "rFvWJRoxDW2f"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Stemming is a heuristic procedure that chops off the ends of words in the hopes of getting it right most of the time, and it frequently includes the removal of derivational affixes.\n",
        "\n",
        "Lemmatization usually refers to doing things correctly using a vocabulary and morphological study of words, with the goal of removing only inflectional endings and returning the base or dictionary form of a word, known as the lemma. Stemming might yield only s when confronted with the token saw, whereas lemmatization might try to return either see or saw depending on whether the token was used as a verb or a noun.\n",
        "\n",
        "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n",
        "Python NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_uuid": "6952be9b9e3279adcf5bfc227bb9bc8aa0ff6cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pDcY9IMDW2g",
        "outputId": "32924ddc-795a-4ccc-bed1-5954d93ee75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3ad2a8e9501483b3b1c004030b96b48fa837660e",
        "id": "zclokU7UDW2g"
      },
      "source": [
        "In the above code output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. \n",
        "\n",
        "You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "_uuid": "399fe7f05121a2c91b766650c71808b90cf58943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqejzstUDW2h",
        "outputId": "5cda01ce-19c4-41f6-94aa-1ddcabd0e5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization with POS Tagging \n",
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 be                  \n",
            "running             run                 \n",
            "and                 and                 \n",
            "eating              eat                 \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 have                \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swim                \n",
            "after               after               \n",
            "playing             play                \n",
            "long                long                \n",
            "hours               hours               \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ],
      "source": [
        "print(\"Lemmatization with POS Tagging \")\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f3678c45b4e1d35213f684304e3e716dc99aad1e",
        "id": "xdZSSuIrDW2h"
      },
      "source": [
        "## Token Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_uuid": "cf92283a81b0e540cc001699fd6c25068285fd6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNYEGLYiDW2i",
        "outputId": "5c993898-3d0a-4dd7-df35-73bf64eefab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 18), ('of', 10), ('to', 9), ('we', 9), (',', 8), ('.', 7), ('is', 6), ('will', 6), ('our', 6), ('which', 5)]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "article = \"\"\"If men wish to live, then they are forced to kill others. The entire struggle for survival is a\n",
        "conquest of the means of existence, which in turn results in the elimination of others from these\n",
        "same sources of subsistence. As long as there are peoples on this earth, there will be nations\n",
        "against nations and they will be forced to protect their vital rights in the same way as the\n",
        "individual is forced to protect his rights.\n",
        "One is either the hammer or the anvil. We confess that it is our purpose to prepare the\n",
        "German people again for the role of the hammer. We admit freely and openly that if our\n",
        "movement is victorious, we will be concerned day and night with the question of how to produce\n",
        "the armed forces, which are forbidden us by the peace treaty [Treaty of Versailles]. We solemnly\n",
        "confess that we consider everyone a scoundrel who does not try day and night to figure out a way\n",
        "to violate this treaty, for we have never recognized this treaty...We will take every step which\n",
        "strengthens our arms, which augments the number of our forces, and which increases the strength\n",
        "of our people. We confess further that we will dash anyone to pieces who should dare hinder us\n",
        "in this undertaking...Our rights will be protected only when the German Reich [country] is again\n",
        "supported by the point of the German dagger... \"\"\"\n",
        "#print()\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Day 2 - Stemming and Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}